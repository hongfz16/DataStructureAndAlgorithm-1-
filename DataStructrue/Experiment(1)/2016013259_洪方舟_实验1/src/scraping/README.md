# 加分项说明
## 一、目标
1. 通过抓取大量网页内容进行词频统计
2. 扩充词库，优化中文分词的效果
## 二、实现
1. 通过铁甲论坛帖子导航页面直接获取大量帖子url
2. 通过BeautifulSoup提取发帖内容
3. 数据清洗，去除除了中文字符以外的全部字符
4. 通过简单的n-gram模型分词，进行词频统计
5. 取出词频较高的词加入到词典中
## 三、测试方法
1. 运行环境
	- 编程语言：python3.x
	- 依赖包：BeautifulSoup4
2. 运行方法
	- 请在网络通畅的环境下运行`scraping.py`
	- 共生成三个文件：
		1. `contents.txt` 包含所爬取的所有内容（默认情况下只爬取200个页面，方便测试）
		2. `segment.txt` 使用n-gram模型生成的所有待统计词组
		3. `count.txt` 词频统计结果
## 四、结果说明
1. 在同目录`results`文件夹下存放着两个文本文件，`contents.txt`为抓取20000个网页后提取的全部发帖内容信息（已经进行过数据清洗）
2. `sorted.txt`文件为词频分词后对词频进行排序后的结果